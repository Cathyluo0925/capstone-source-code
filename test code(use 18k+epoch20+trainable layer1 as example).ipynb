{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a00f4c-e57a-43b7-b86c-62c8eccdf92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import metrics  \n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from keras.applications import resnet\n",
    "class SiameseModel(Model):\n",
    "    \"\"\"The Siamese Network model with a custom training and testing loops.\n",
    "\n",
    "    Computes the triplet loss using the three embeddings produced by the\n",
    "    Siamese Network.\n",
    "\n",
    "    The triplet loss is defined as:\n",
    "       L(A, P, N) = max(‖f(A) - f(P)‖² - ‖f(A) - f(N)‖² + margin, 0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, siamese_network, margin=0.5,**kwargs):\n",
    "        super(SiameseModel, self).__init__(**kwargs)\n",
    "        self.siamese_network = siamese_network\n",
    "        self.margin = margin\n",
    "        self.loss_tracker = metrics.Mean(name=\"loss\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.siamese_network(inputs)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # GradientTape is a context manager that records every operation that\n",
    "        # you do inside. We are using it here to compute the loss so we can get\n",
    "        # the gradients and apply them using the optimizer specified in\n",
    "        # `compile()`.\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self._compute_loss(data)\n",
    "\n",
    "        # Storing the gradients of the loss function with respect to the\n",
    "        # weights/parameters.\n",
    "        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)\n",
    "\n",
    "        # Applying the gradients on the model using the specified optimizer\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(gradients, self.siamese_network.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Let's update and return the training loss metric.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        loss = self._compute_loss(data)\n",
    "\n",
    "        # Let's update and return the loss metric.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def _compute_loss(self, data):\n",
    "        # The output of the network is a tuple containing the distances\n",
    "        # between the anchor and the positive example, and the anchor and\n",
    "        # the negative example.\n",
    "        ap_distance, an_distance = self.siamese_network(data)\n",
    "\n",
    "        # Computing the Triplet Loss by subtracting both distances and\n",
    "        # making sure we don't get a negative value.\n",
    "        loss = ap_distance - an_distance\n",
    "        loss = tf.maximum(loss + self.margin, 0.0)\n",
    "        return loss\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We need to list our metrics here so the `reset_states()` can be\n",
    "        # called automatically.\n",
    "        return [self.loss_tracker]\n",
    "\n",
    "    # Add get_config and from_config for serialization\n",
    "    def get_config(self):\n",
    "        config = super(SiameseModel, self).get_config()\n",
    "        config.update({\n",
    "            \"siamese_network\": self.siamese_network.get_config() if hasattr(self.siamese_network, \"get_config\") else None\n",
    "        })\n",
    "        return config\n",
    "        \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        siamese_network = config.pop(\"siamese_network\")  # Extract custom objects\n",
    "        return cls(siamese_network=siamese_network, **config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392f4b92-5d1b-4d73-8fc9-0d4f3e94fc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_shape = (200,200)\n",
    "class DistanceLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    This layer is responsible for computing the distance between the anchor\n",
    "    embedding and the positive embedding, and the anchor embedding and the\n",
    "    negative embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(DistanceLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, anchor, positive, negative):\n",
    "        # 使用 TensorFlow 的 reduce_sum 来计算距离\n",
    "        ap_distance = tf.reduce_sum(tf.square(anchor - positive), axis=-1)\n",
    "        an_distance = tf.reduce_sum(tf.square(anchor - negative), axis=-1)\n",
    "        return (ap_distance, an_distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ce7cbf-54ba-43dc-9c73-2f28b1ad9655",
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_shape = (200,200)  # 假设图像是 RGB 格式，大小为 200x200\n",
    "\n",
    "base_cnn = resnet.ResNet50(\n",
    "    weights=\"imagenet\", input_shape=target_shape + (3,), include_top=False\n",
    ")\n",
    "#print(base_cnn.layers)\n",
    "flatten = layers.Flatten()(base_cnn.output)\n",
    "dense1 = layers.Dense(512, activation=\"relu\")(flatten)\n",
    "dense1 = layers.BatchNormalization()(dense1)\n",
    "dense2 = layers.Dense(256, activation=\"relu\")(dense1)\n",
    "dense2 = layers.BatchNormalization()(dense2)\n",
    "output = layers.Dense(256)(dense2)\n",
    "\n",
    "embedding = Model(base_cnn.input, output, name=\"Embedding\")\n",
    "\n",
    "trainable = False\n",
    "#adjust the number of trainable layers\n",
    "for layer in base_cnn.layers:\n",
    "    if layer.name == \"conv5_block1_out\":\n",
    "        trainable = True\n",
    "    layer.trainable = trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf86db55-23f0-4538-a4f7-610c326c5027",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "anchor_input = layers.Input(name=\"anchor\", shape=target_shape + (3,))\n",
    "positive_input = layers.Input(name=\"positive\", shape=target_shape + (3,))\n",
    "negative_input = layers.Input(name=\"negative\", shape=target_shape + (3,))\n",
    "\n",
    "distances = DistanceLayer()(\n",
    "    embedding(resnet.preprocess_input(anchor_input)),\n",
    "    embedding(resnet.preprocess_input(positive_input)),\n",
    "    embedding(resnet.preprocess_input(negative_input)),\n",
    ")\n",
    "\n",
    "# 假设你已经定义了一个 SiameseNetwork 类\n",
    "siamese_network = Model(\n",
    "    inputs=[anchor_input, positive_input, negative_input], outputs=distances\n",
    ")\n",
    "\n",
    "# 然后将它作为参数传递给 SiameseModel\n",
    "siamese_model = SiameseModel(siamese_network=siamese_network)\n",
    "\n",
    "input_shape = (None, 200, 200, 3)  # 这里你需要使用与你的数据一致的输入形状\n",
    "siamese_model.build(input_shape)\n",
    "# 如果加载权重，可以这样做\n",
    "siamese_model.load_weights('siamese_18k_epoch20_trainable1_weights.weights.h5')\n",
    "\n",
    "# 查看加载后的模型结构\n",
    "siamese_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32358d1a-2e06-4e08-b988-b13ba97b377f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test two dances\n",
    "#for testing dataset\n",
    "from pathlib import Path\n",
    "cache_dir = Path(Path.home()) / \"autodl-tmp\"\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d26d74-6292-4da5-8256-b2a9dec171ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install python-docx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823c770b-670e-4068-a16f-0a17f404edd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unzip all the test data\n",
    "!unzip -oq test6a_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test6c_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test6e_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test6f_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test6g_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test6h_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test6i_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test6j_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test6k_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test6l_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test3d_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test3e_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test3g_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test3h_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test4c_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test4d_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test4e_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test4f_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test5a_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test5b_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test5c_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test5d_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test5g_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test5h_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test5k_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test5l_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test7a_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test7b_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test7c_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test7d_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test7e_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test7f_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test7g_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test7h_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test7i_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test7j_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test7k_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test7l_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test7m_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test7n_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test7o_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test7p_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test7q_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test7r_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test7s_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test7t_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test8a_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test8b_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test8c_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test8d_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test8e_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test8f_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test8g_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test8h_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test2a_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq test2b_outcome_integrate.zip -d $cache_dir\n",
    "!unzip -oq emma_11_30s_integrate_remedy.zip -d $cache_dir\n",
    "!unzip -oq nb_lqx_29s_case2_integrate_remedy.zip -d $cache_dir\n",
    "!unzip -oq nb_lqx_29s_integrate_remedy.zip -d $cache_dir\n",
    "#all:59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2488c007-a31b-4ae7-a29a-3b8c71fa3477",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import re\n",
    "from docx import Document\n",
    "\n",
    "# Define paths for all cases\n",
    "cases = [\n",
    "    {\"anchor\": 'autodl-tmp/emma_11_30s_integrate_remedy', \"positive\":'autodl-tmp/nb_lqx_29s_case2_integrate2_remedy'},\n",
    "    {\"anchor\": 'autodl-tmp/test2a_outcome_integrate', \"positive\": 'autodl-tmp/test2b_outcome_integrate'},\n",
    "    \n",
    "    {\"anchor\": 'autodl-tmp/test3d_outcome_integrate', \"positive\": 'autodl-tmp/test3e_outcome_integrate'},\n",
    "    {\"anchor\": 'autodl-tmp/test3d_outcome_integrate', \"positive\": 'autodl-tmp/test3g_outcome_integrate'},\n",
    "    {\"anchor\": 'autodl-tmp/test3d_outcome_integrate', \"positive\": 'autodl-tmp/test3h_outcome_integrate'},\n",
    "    \n",
    "    {\"anchor\": 'autodl-tmp/test4c_outcome_integrate', \"positive\": 'autodl-tmp/test4d_outcome_integrate'},    \n",
    "    {\"anchor\": 'autodl-tmp/test4e_outcome_integrate', \"positive\": 'autodl-tmp/test4f_outcome_integrate'},\n",
    "    \n",
    "    {\"anchor\": 'autodl-tmp/test5a_outcome_integrate', \"positive\": 'autodl-tmp/test5b_outcome_integrate'},\n",
    "    {\"anchor\": 'autodl-tmp/test5c_outcome_integrate', \"positive\": 'autodl-tmp/test5d_outcome_integrate'},\n",
    "    {\"anchor\": 'autodl-tmp/test5g_outcome_integrate', \"positive\": 'autodl-tmp/test5h_outcome_integrate'},\n",
    "    {\"anchor\": 'autodl-tmp/test5k_outcome_integrate', \"positive\": 'autodl-tmp/test5l_outcome_integrate'},\n",
    "    \n",
    "    {\"anchor\": 'autodl-tmp/test6a_outcome_integrate', \"positive\": 'autodl-tmp/test6c_outcome_integrate'},\n",
    "    {\"anchor\": 'autodl-tmp/test6e_outcome_integrate', \"positive\": 'autodl-tmp/test6f_outcome_integrate'},\n",
    "    {\"anchor\": 'autodl-tmp/test6g_outcome_integrate', \"positive\": 'autodl-tmp/test6h_outcome_integrate'},\n",
    "    {\"anchor\": 'autodl-tmp/test6i_outcome_integrate', \"positive\": 'autodl-tmp/test6j_outcome_integrate'},\n",
    "    {\"anchor\": 'autodl-tmp/test6k_outcome_integrate', \"positive\": 'autodl-tmp/test6l_outcome_integrate'},\n",
    "    \n",
    "    {\"anchor\": 'autodl-tmp/test7a_outcome_integrate', \"positive\": 'autodl-tmp/test7b_outcome_integrate'},\n",
    "    {\"anchor\": 'autodl-tmp/test7c_outcome_integrate', \"positive\": 'autodl-tmp/test7d_outcome_integrate'},\n",
    "    {\"anchor\": 'autodl-tmp/test7e_outcome_integrate', \"positive\": 'autodl-tmp/test7f_outcome_integrate'},\n",
    "    {\"anchor\": 'autodl-tmp/test7g_outcome_integrate', \"positive\": 'autodl-tmp/test7h_outcome_integrate'},\n",
    "    {\"anchor\": 'autodl-tmp/test7i_outcome_integrate', \"positive\": 'autodl-tmp/test7j_outcome_integrate'},\n",
    "    {\"anchor\": 'autodl-tmp/test7k_outcome_integrate', \"positive\": 'autodl-tmp/test7l_outcome_integrate'},\n",
    "    {\"anchor\": 'autodl-tmp/test7m_outcome_integrate', \"positive\": 'autodl-tmp/test7n_outcome_integrate'},\n",
    "    {\"anchor\": 'autodl-tmp/test7o_outcome_integrate', \"positive\": 'autodl-tmp/test7p_outcome_integrate'},\n",
    "    {\"anchor\": 'autodl-tmp/test7q_outcome_integrate', \"positive\": 'autodl-tmp/test7r_outcome_integrate'},\n",
    "    {\"anchor\": 'autodl-tmp/test7s_outcome_integrate', \"positive\": 'autodl-tmp/test7t_outcome_integrate'},\n",
    "    \n",
    "    {\"anchor\": 'autodl-tmp/test8a_outcome_integrate', \"positive\": 'autodl-tmp/test8b_outcome_integrate'},\n",
    "    {\"anchor\": 'autodl-tmp/test8c_outcome_integrate', \"positive\": 'autodl-tmp/test8d_outcome_integrate'},\n",
    "    {\"anchor\": 'autodl-tmp/test8e_outcome_integrate', \"positive\": 'autodl-tmp/test8f_outcome_integrate'},\n",
    "    {\"anchor\": 'autodl-tmp/test8g_outcome_integrate', \"positive\": 'autodl-tmp/test8h_outcome_integrate'}\n",
    "    \n",
    "    # Add more cases as needed\n",
    "]\n",
    "\n",
    "\n",
    "# Function to sort file names numerically\n",
    "def numeric_sort(filenames):\n",
    "    return sorted(filenames, key=lambda x: int(re.findall(r'\\d+', x)[0]))\n",
    "\n",
    "\n",
    "\n",
    "# Function to process a single case\n",
    "def process_case(anchor_folder, positive_folder, case_name, window_size=5, step=1, threshold=1.8):\n",
    "    # Load, filter, and sort image file names numerically\n",
    "    anchor_images = numeric_sort([f for f in os.listdir(anchor_folder) if f.endswith(('.jpg', '.png'))])\n",
    "    positive_images = numeric_sort([f for f in os.listdir(positive_folder) if f.endswith(('.jpg', '.png'))])\n",
    "\n",
    "    # Convert to full paths\n",
    "    anchor_images = [os.path.join(anchor_folder, f) for f in anchor_images]\n",
    "    positive_images = [os.path.join(positive_folder, f) for f in positive_images]\n",
    "\n",
    "    # Generate overlapping groups\n",
    "    #anchor_groups = generate_overlapping_groups(anchor_images, window_size, step)\n",
    "    #positive_groups = generate_overlapping_groups(positive_images, window_size, step)\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    output_dir = './outputnew_18k_epoch20_train1'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Create a new Document for the current case\n",
    "    doc = Document()\n",
    "    doc.add_heading(f'Testing Results: {case_name}', 0)\n",
    "\n",
    "    start_index_p = 0\n",
    "    start_index_a = 0\n",
    "    # Start testing\n",
    "    for j,positive_path in enumerate(positive_images):\n",
    "        for i,anchor_path in enumerate(anchor_images):\n",
    "            \n",
    "            doc.add_heading(f'Testing Positive Image {j+1} with Anchor Image {i+1}', level=1)\n",
    "            print (f'Testing Positive Image {j+1} with Anchor Image {i+1}')\n",
    "           \n",
    "            anchor_image = tf.image.decode_jpeg(tf.io.read_file(anchor_path), channels=3)\n",
    "            anchor_image = tf.image.resize(anchor_image, [200, 200])\n",
    "            anchor_image = resnet.preprocess_input(anchor_image)\n",
    "            anchor_image = tf.expand_dims(anchor_image, axis=0)  # Add batch dimension\n",
    "\n",
    "            positive_image = tf.image.decode_jpeg(tf.io.read_file(positive_path), channels=3)\n",
    "            positive_image = tf.image.resize(positive_image, [200, 200])\n",
    "            positive_image = resnet.preprocess_input(positive_image)\n",
    "            positive_image = tf.expand_dims(positive_image, axis=0)\n",
    "        \n",
    "            # Generate embeddings\n",
    "            embedding1 = embedding(resnet.preprocess_input(positive_image))\n",
    "            embedding2 = embedding(resnet.preprocess_input(anchor_image))\n",
    "\n",
    "            # Calculate Euclidean distance\n",
    "            distance1 = tf.norm(embedding1 - embedding2, ord='euclidean').numpy()\n",
    "\n",
    "                # Add to document\n",
    "            doc.add_paragraph(f'Predicted Euclidean Distance: {distance1}')\n",
    "            print(f'Predicted Euclidean Distance: {distance1}')\n",
    "                \n",
    "            # Save document for the current anchor-positive pair\n",
    "            doc_name = os.path.join(output_dir, f'{case_name}_results.docx')\n",
    "            doc.save(doc_name)\n",
    "            #print(f\"Results for {case_name} saved as {doc_name}\")\n",
    "\n",
    "# Iterate through all cases\n",
    "for case in cases:\n",
    "    case_name = f\"{os.path.basename(case['anchor']).split('_')[0]}_vs_{os.path.basename(case['positive']).split('_')[0]}\"\n",
    "    print(f\"Processing case: {case_name}\")\n",
    "    process_case(case[\"anchor\"], case[\"positive\"], case_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
